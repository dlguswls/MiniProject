{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "211003_DALL-E.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FreX8Ag6kzu"
      },
      "source": [
        "# 1. Text2Image \n",
        "> 우리가 생성한 이야기를 담고있는 이미지 생성\n",
        "\n",
        "* 후보 알고리즘: StackGAN, Text2Scene, DALL-E 등 \n",
        "* 선정 알고리즘: __DALL-E__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg8YjSfCH1j3"
      },
      "source": [
        "### 0. DALL-E\n",
        " * 코드 깃헙: https://github.com/ai-coodinator/DALL-E/blob/main/DALL_E.ipynb\n",
        " * 코드 블로그: https://stevenhickson.blogspot.com/2021/03/test.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMaOUSp567eD",
        "outputId": "c955cae3-f918-49a6-b329-23b13c7ad35f"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-8b70f006-3812-dfc4-c640-c7cbdd9d4459)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlEw4JyUEFBC",
        "outputId": "24ea9ef7-c0e6-44c7-cc90-d2c6ca20274f"
      },
      "source": [
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "!pip install DALL-E\n",
        "!pip install ftfy\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "%cd /content/CLIP/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (735.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4 MB 15 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 80 kB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (3.10.0.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu101) (7.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=9db4025c1da440692dc938e9214f1b4f71a10084aa30b6263e79654efd1cc8f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built ftfy\n",
            "Installing collected packages: torch, torchvision, ftfy\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pZP4ivO-wNR"
      },
      "source": [
        "# torch seed 고정\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "torch.manual_seed(2021)\n",
        "torch.cuda.manual_seed(2021)\n",
        "torch.cuda.manual_seed_all(2021)\n",
        "np.random.seed(2021)\n",
        "cudnn.benchmark = False\n",
        "cudnn.deterministic = True\n",
        "random.seed(2021)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Avt3epOEM-c"
      },
      "source": [
        "### 1. Preparing\n",
        "* 이미지 스케일 조정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhQrsdyBEFB4"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import imageio\n",
        "from IPython import display\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import glob\n",
        "from google.colab import output\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "# probably don't mess with this unless you're changing generator size\n",
        "im_shape = [512, 512, 3]\n",
        "sideX, sideY, channels = im_shape\n",
        "\n",
        "def displ(img, pre_scaled=True):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1, 2, 0))\n",
        "  if not pre_scaled:\n",
        "    img = scale(img, 48*4, 32*4)\n",
        "  imageio.imwrite(str(3) + '.png', np.array(img))\n",
        "  return display.Image(str(3)+'.png')\n",
        "\n",
        "def gallery(array, ncols=2):\n",
        "    nindex, height, width, intensity = array.shape\n",
        "    nrows = nindex//ncols\n",
        "    assert nindex == nrows*ncols\n",
        "    # want result.shape = (height*nrows, width*ncols, intensity)\n",
        "    result = (array.reshape(nrows, ncols, height, width, intensity)\n",
        "              .swapaxes(1,2)\n",
        "              .reshape(height*nrows, width*ncols, intensity))\n",
        "    return result\n",
        "\n",
        "def card_padded(im, to_pad=3):\n",
        "  return np.pad(np.pad(np.pad(im, [[1,1], [1,1], [0,0]],constant_values=0), [[2,2], [2,2], [0,0]],constant_values=1),\n",
        "            [[to_pad,to_pad], [to_pad,to_pad], [0,0]],constant_values=0)\n",
        "\n",
        "def get_all(img):\n",
        "  img = np.transpose(img, (0,2,3,1))\n",
        "  cards = np.zeros((img.shape[0], sideX+12, sideY+12, 3))\n",
        "  for i in range(len(img)):\n",
        "    cards[i] = card_padded(img[i])\n",
        "  print(img.shape)\n",
        "  cards = gallery(cards)\n",
        "  imageio.imwrite(str(3) + '.png', np.array(cards))\n",
        "  return display.Image(str(3)+'.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aapekazfEyLW"
      },
      "source": [
        "### 2. Perceptor\n",
        "* CLIP 모델 이용\n",
        "* 사전 훈련된 모델을 기반으로 제로샷 전이 학습 -> 이미지에 캡션 제공"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpvWooKEEFC0"
      },
      "source": [
        "import clip\n",
        "clip.available_models()\n",
        "\n",
        "# Load the model\n",
        "perceptor, preprocess = clip.load('ViT-B/32', jit=True)\n",
        "perceptor = perceptor.eval() # evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl6cxqR1GnUl"
      },
      "source": [
        "### 3. Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sExEuktCEFEZ"
      },
      "source": [
        "import io\n",
        "import os, sys\n",
        "import requests\n",
        "import PIL\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from dall_e import map_pixels, unmap_pixels, load_model\n",
        "\n",
        "target_image_size = sideX\n",
        "\n",
        "def preprocess(img):\n",
        "    s = min(img.size)\n",
        "    \n",
        "    if s < target_image_size:\n",
        "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
        "        \n",
        "    r = target_image_size / s\n",
        "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
        "    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n",
        "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
        "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
        "    return map_pixels(img)\n",
        "\n",
        "model = load_model(\"https://cdn.openai.com/dall-e/decoder.pkl\", 'cuda') # DALL-E: transformer의 decoder만 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnHetf1OHibJ"
      },
      "source": [
        "### 4. Text input\n",
        "* 기본/A fairy tale of/An illustration of 등 text input에 워딩을 바꿔보면서 학습\n",
        "* 뭐가 더 '동화책스럽다'는 딱히 없는 듯(운빨임)\n",
        "* dall-e mini를 버리는 상황이라면 문장 워딩을 조금씩 바꾼 걸 후보군으로 생성해도 되지 않을까? \n",
        "* 혹은 seed 빨도 많이 받으니까 seed에 따른 생성결과를 후보로 줄 수도 있을 듯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_n64PKSEFHC"
      },
      "source": [
        "text_input = \"golden ring was hanging on the wall of room, and it glittered with jewels in his pocket. when he came to the door they saw beautiful princess who wear gold dress as well!\"\n",
        "tau_value = 1.0 # non-negative scalar temperature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_z18sGqHkhP"
      },
      "source": [
        "### 5. Latent coordinate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GQCWBJuEFJm"
      },
      "source": [
        "class Pars(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pars, self).__init__()\n",
        "        self.normu = torch.nn.Parameter(torch.randn(1, 8192, 64, 64).cuda())\n",
        "\n",
        "    def forward(self):\n",
        "      #normu = torch.nn.functional.gumbel_softmax(self.normu.view(1, 8192, -1), dim=-1).view(1, 8192, 64, 64)\n",
        "      normu = torch.nn.functional.gumbel_softmax(self.normu.view(1, 8192, -1), dim=-1, tau=tau_value).view(1, 8192, 64, 64) \n",
        "      return normu\n",
        "\n",
        "lats = Pars().cuda()\n",
        "mapper = [lats.normu]\n",
        "optimizer = torch.optim.Adam([{'params': mapper, 'lr': .1}]) \n",
        "#eps = 0 # epsilon\n",
        "tx = clip.tokenize(text_input)\n",
        "t = perceptor.encode_text(tx.cuda()).detach().clone()\n",
        "nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)) # 이 값을 조정해보고 싶은데 감이 안와서 못 건드리겠음 -> 평균/표준편차 -> 고정 \n",
        "\n",
        "with torch.no_grad():\n",
        "  mult = 1\n",
        "  al = unmap_pixels(torch.sigmoid(model(lats()).cpu().float())).numpy()\n",
        "  for allls in al:\n",
        "    displ(allls[:3])\n",
        "    print('\\n')\n",
        "  # print(torch.topk(lats().view(1, 8192, -1), k=3, dim=-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci9cPLTLHnJa"
      },
      "source": [
        "### 6. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "agIEDStNEFMS"
      },
      "source": [
        "def checkin(loss):\n",
        "  print('''########################################################## ''',loss, '\\n',itt)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    al = unmap_pixels(torch.sigmoid(model(lats())[:, :3]).cpu().float()).numpy()\n",
        "  for allls in al:\n",
        "    displ(allls)\n",
        "    display.display(display.Image(str(3)+'.png'))\n",
        "    print('\\n')\n",
        "  # the people spoke and they love \"ding\"\n",
        "  # output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')\n",
        "\n",
        "def ascend_txt():\n",
        "  out = unmap_pixels(torch.sigmoid(model(lats())[:, :3].float()))\n",
        "  cutn = 64 ## improves quality -> 수정 \n",
        "  p_s = []\n",
        "  for ch in range(cutn):\n",
        "    size = int(sideX*torch.zeros(1,).normal_(mean=.8, std=.3).clip(.5, .98))\n",
        "    offsetx = torch.randint(0, sideX - size, ())\n",
        "    offsety = torch.randint(0, sideX - size, ())\n",
        "    apper = out[:, :, offsetx:offsetx + size, offsety:offsety + size]\n",
        "    apper = torch.nn.functional.interpolate(apper, (224,224), mode='bilinear') ## 해상도 관련? \n",
        "    p_s.append(apper)\n",
        "  into = torch.cat(p_s, 0)\n",
        "  # into = torch.nn.functional.interpolate(out, (224,224), mode='nearest')\n",
        "  into = nom(into)\n",
        "  iii = perceptor.encode_image(into)\n",
        "  llls = lats()\n",
        "  lat_l = 0\n",
        "  return [lat_l, 10*-torch.cosine_similarity(t, iii).view(-1, 1).T.mean(1)]\n",
        "\n",
        "def train(i):\n",
        "  loss1 = ascend_txt() \n",
        "  loss = loss1[0] + loss1[1]\n",
        "  loss = loss.mean()\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "    \n",
        "  if itt % 50 == 0:\n",
        "    checkin(loss1)\n",
        "    shutil.copy('./3.png', './images/%s.png'%str(int(itt/100)).zfill(6))\n",
        "\n",
        "import shutil\n",
        "\n",
        "if os.path.isdir('images'):\n",
        "    shutil.rmtree('images')\n",
        "os.makedirs('images', exist_ok=True)\n",
        "\n",
        "itt = 0\n",
        "for asatreat in range(2021):\n",
        "  train(itt)\n",
        "  itt+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTzc27l_Ho7f"
      },
      "source": [
        "### 7. Make movie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DASRyuMWEFQg"
      },
      "source": [
        "if os.path.exists('./output.mp4'):\n",
        "   os.remove('./output.mp4')\n",
        "\n",
        "!ffmpeg -r 2 -i images/%06d.png -vcodec libx264 -pix_fmt yuv420p output.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdarDs2B6tct"
      },
      "source": [
        "# 2. Style Transfer \n",
        "> 이미지를 원하는 그림책 분위기에 맞게 변형 <br>\n",
        "> DALL-E에서 그림책 이미지를 잘 생성한다면 생략 가능한 단계\n",
        "\n",
        "* 후보 알고리즘: CycleGAN, CartoonGAN, GANILLA 등\n",
        "* 선정 알고리즘: 미정 -> __안 하는 방향으로 결정__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0KjjHfaFIiH"
      },
      "source": [
        "# 3. Reference\n",
        "1. Transformer 논문 리뷰\n",
        "* 올려주신 자료\n",
        "* 쉽게: https://lv99.tistory.com/26\n",
        "* 자세히: https://kmhana.tistory.com/28\n",
        "2. DALL-E 논문 리뷰\n",
        "* 쉽게: https://jiho-ml.com/weekly-nlp-40/\n",
        "* 자세히: https://littlefoxdiary.tistory.com/74\n",
        "3. CartoonGAN 논문 리뷰\n",
        "* GAN: https://tobigs.gitbook.io/tobigs/deep-learning/computer-vision/gan-generative-adversarial-network\n",
        "* CartoonGAN: https://blog.diyaml.com/teampost/Improving-CartoonGAN/\n",
        "4. Code\n",
        "* DALL-E: https://github.com/ai-coodinator/DALL-E\n",
        "* CartoonCAN: https://github.com/TobiasSunderdiek/cartoon-gan\n",
        "* GANILLA: \n",
        "https://neurohive.io/en/news/ganilla-gan-network-trained-to-generate-children-s-book-illustrations/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2u375hBFmml"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}